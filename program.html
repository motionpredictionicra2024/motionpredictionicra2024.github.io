<section id="program"class="wrapper alt style4">
					<div class="inner">
						<h3>Program (tentative)</h3>
						<div class="table-wrapper">
							<table>
								<thead>
									<tr>
										<th>Time</th>
										<th>Speaker</th>
										<th>Title</th>
										<th>Abstract</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>9:00-9:15 (JST)</td>
										<td>Organizers</td>
										<td>Intro</td>
										<td></td>
									</tr>
									<tr>
										<td>9:15-9:45 (JST)</td>
										<td><strong><a href="https://eetchen.github.io/">Tao Chen</a></strong>, Fudan University</td>
										<td>MotionGPT (tentative)</td>
										<td>Abstract</td>
									</tr>
									<tr>
										<td>9:45-10:15 (JST)</td>
										<td><strong><a href="https://research.nvidia.com/person/marco-pavone">Marco Pavone</a></strong>, Stanford University, nVidia</td>
										<td>Revolutionizing AV Development With Foundation Models</td>
										<td>Foundation models, trained on vast and diverse data encompassing the human experience, are at the heart of the ongoing AI revolution influencing the way we create, problem solve, and work. These models, and the lessons learned from their construction, can also be applied to the way we develop a similarly transformative technology, autonomous vehicles. In this talk Iâ€™ll highlight recent research efforts toward rethinking elements of an AV program both in the vehicle and in the data center, with an emphasis on (1) leveraging diverse data sources for long-tail safety evaluation, (2) composing ingredients for universal and controllable end-to-end simulation, and (3) building the self-accelerating data flywheels that will enable scaling AV learning to new frontiers of autonomous reasoning and generalization.</td>
									</tr>
									<tr>
										<td>10:15-10:45 (JST)</td>
										<td><strong><a href="https://aroitberg.github.io/">Alina Roitberg</a></strong>, University of Stuttgart</td>
										<td>Towards resource-efficient and uncertainty-aware driver behaviour understanding and maneuver prediction</td>
										<td>This talk will explore recent advances in video-based driver observation techniques aimed at creating adaptable, resource- and data-efficient, as well as uncertainty-aware models for in-vehicle monitoring and maneuver prediction. Topics covered will include: (1) an overview state-of-the-art  methods and  public datasets for driver activity analysis  (2) the importance of adaptability in driver observation systems to cater to new situations (environments, vehicle types, driver behaviours) as well as strategies for addressing such open world tasks, and (3) incorporating uncertainty-aware approaches, vital for robust and safe decision-making. The talk will conclude with a discussion of future research directions and the potential applications of this technology, such as improving driver safety and improving the overall driving experience.</td>
									</tr>
									<tr>
										<td>10:45-11:15 (JST)</td>
										<td>Coffee break</td>
										<td> </td>
										<td> </td>
									</tr>
									<tr>
										<td>11:15-11:45 (JST)</td>
										<td><strong><a href="https://research.manchester.ac.uk/en/persons/angelo.cangelosi">Angelo Cangelosi</a></strong>, The University of Manchester</td>
										<td>Title</td>
										<td>Abstract</td>
									</tr>
									<tr>
										<td>11:45-12:15 (JST)</td>
										<td><strong><a href="https://www.iit.it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a></strong>, Italian Institute of Technology</td>
										<td>Predictive and Perspective Control of Human-Robot Interaction through Kino-dynamic State Fusion</td>
										<td>Abstract</td>
									</tr>
									<tr>
										<td>12:15-13:30 (JST)</td>
										<td>Lunch break</td>
										<td> </td>
										<td> </td>
									</tr>

									<tr>
										<td>13:30-14:30 (JST)</td>
										<td>Poster Session</td>
										<td> </td>
										<td> </td>
									</tr>

									<tr>
										<td>14:30-15:00 (JST)</td>
										<td><strong><a href="https://www.sfu.ca/computing/people/faculty/mochen.html">Mo Chen</a></strong>, Simon Fraser University</td>
										<td>Long-Term Human Motion Prediction Through Hierarchy, Learning, and Control</td>
										<td>One of the keys to long-term human motion prediction is hierarchy: Just a few high-level actions can encompass a long duration, while the details of human motion at shorter time scales can be inherently encoded in the high-level actions themselves. In this talk, we will discuss two long-term human trajectory prediction frameworks that take advantage of hierarchy. At the high level, we will look at both action spaces that are hand-designed and those that are learned from data. At the low-level, we will examine how details of human motion at shorter time scales can be reconstructed through a combination of control- and learning-based methods.</td>
									</tr>


									<tr>
										<td>15:00-15:30 (JST)</td>
										<td><strong><a href="https://sanjibanc.github.io/"> Sanjiban Choudhury</a></strong>, Cornell University</td>
										<td>Title</td>
										<td>Abstract</td>
									</tr>


									<tr>
										<td>15:30-15:45 (JST)</td>
										<td>Coffe break</td>
										<td> </td>
										<td> </td>
									</tr>
									

									<tr>
										<td>15:45-16:15 (JST)</td>
										<td><strong><a href="https://ariostgx.github.io/website/">Shuhan Tan</a></strong>, The University of Texas at Austin</td>
										<td>Title</td>
										<td>Abstract</td>
									</tr>


									<tr>
										<td>16:15-16:45 (JST)</td>
										<td><strong><a href="https://www.me.columbia.edu/faculty/sunil-agrawal">Sunil K. Agrawal</a></strong>, Columbia University</td>
										<td>Title</td>
										<td>Abstract</td>
									</tr>

									<tr>
										<td>16:45-17:00 (JST)</td>
										<td>Organizers</td>
										<td>Discussion and conclusions</td>
										<td> </td>
									</tr>
									
								</tbody>
							</table>
						</div>
					</div>
				</section>